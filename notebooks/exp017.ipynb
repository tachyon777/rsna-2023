{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp017  \n",
    "[Notion](https://www.notion.so/exp017-92d233032cf349aea43517ad05c88f7d?pvs=4)  \n",
    "高速推論パイプラインの構築  \n",
    "exp012では提出用のパイプラインを構築したが、このnotebookではスコア算出に関係する機能だけを自作データセットから呼び出す.  \n",
    "Copy from: exp012.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Any, Dict, Optional\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# リポジトリtopに移動\n",
    "while os.path.basename(os.getcwd()) != 'rsna-2023':\n",
    "    os.chdir('../')\n",
    "    if os.getcwd() == '/':\n",
    "        raise Exception('Could not find project root directory.')\n",
    "    \n",
    "from src.data_io import load_dicom_series\n",
    "from src.segmentation.dataset import TestDataset as SegTestDataset\n",
    "from src.segmentation.model import load_models as seg_load_models\n",
    "from src.segmentation.trainer import inference as seg_inference\n",
    "from src.classification.dataset import TestDatasetBowelExtra, TestDatasetSolidOrgans\n",
    "from src.image_processing import apply_preprocess, crop_organ, kidney_split, resize_volume, apply_postprocess, kidney_specific, resize_3d, resize_1d\n",
    "from src.classification.model import load_models as cls_load_models\n",
    "from src.classification.trainer import inference as cls_inference\n",
    "from src.metrics import score, create_training_solution\n",
    "from src.classification.dataset import load_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG_INF:\n",
    "    exp_name = 'exp_017'\n",
    "    # evaluation時：'train', submission時：'test'\n",
    "    phase = 'train'\n",
    "    base_dir = 'data/rsna-2023-abdominal-trauma-detection'\n",
    "    image_dir = f'data/rsna-2023-abdominal-trauma-detection/{phase}_images'\n",
    "    # dataframeはこのconfigにもたせ、phaseで対応できるようにする.\n",
    "    if phase == 'train':\n",
    "        df = pd.read_csv(os.path.join(base_dir, 'train.csv'))\n",
    "    elif phase == 'test':\n",
    "        df = pd.read_csv(os.path.join(base_dir, 'sample_submission.csv'))\n",
    "    df_series_meta = pd.read_csv(os.path.join(base_dir, f'{phase}_series_meta.csv'))\n",
    "    image_size = (512, 512)\n",
    "    # sample submissionで極端にスライス数が少ない場合があるため対応.\n",
    "    min_slices = 10\n",
    "    # 推論時間制限のため\n",
    "    max_slices = 500\n",
    "    max_series = 2\n",
    "    model_save_dir = \"outputs\"\n",
    "\n",
    "class CFG_LSK:\n",
    "    exp_name = 'exp_014'\n",
    "    # model config\n",
    "    # timm backbone\n",
    "    backbone = 'efficientnet-b4'\n",
    "    n_ch = 1\n",
    "    expand_ch_dim = True\n",
    "    # n_class: healthy, low, high\n",
    "    n_class = 3\n",
    "    # hyper params\n",
    "    init_lr = 1e-4\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    image_size = (128, 128, 128)\n",
    "    batch_size = 64\n",
    "    amp = True\n",
    "    eps = 1e-6\n",
    "    n_epoch = 20\n",
    "    pretrain = True\n",
    "    freeze_epochs = 1\n",
    "    noaug_epochs = 1\n",
    "    # fold config\n",
    "    n_fold = 6\n",
    "    include_evaluation = False\n",
    "    train_folds = 1\n",
    "    # path\n",
    "    image_dir = \"data/dataset002\"\n",
    "    model_save_dir = \"outputs\"\n",
    "    # other config\n",
    "    seed = 42\n",
    "    num_workers = 0\n",
    "    num_gpus = 2\n",
    "    progress_bar = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CFG_BE:\n",
    "    exp_name = 'exp_011'\n",
    "    # model config\n",
    "    # timm backbone\n",
    "    backbone = 'efficientnet_b4'\n",
    "    # n_ch: z軸方向のスライス数\n",
    "    n_ch = 1 # support only 1\n",
    "    expand_ch_dim = False\n",
    "    # n_class: bowel_injury, extravasation\n",
    "    n_class = 2\n",
    "    label_smoothing = None #Optional(float)\n",
    "    # hyper params\n",
    "    init_lr = 5e-5\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    image_size = (512, 512)\n",
    "    batch_size = 64\n",
    "    amp = True\n",
    "    n_epoch = 20\n",
    "    iteration_per_epoch = 100\n",
    "    pretrain = True\n",
    "    freeze_epochs = 1\n",
    "    noaug_epochs = 1\n",
    "    # fold config\n",
    "    n_fold = 6\n",
    "    include_evaluation = False\n",
    "    train_folds = 1\n",
    "    # path\n",
    "    image_dir = \"data/dataset001\"\n",
    "    model_save_dir = \"outputs\"\n",
    "    # other config\n",
    "    seed = 42\n",
    "    num_workers = 0\n",
    "    num_gpus = 2\n",
    "    progress_bar = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organs dict (for SEG and LSK models)\n",
    "organ_index_dict_inv = {\n",
    "    0: 'liver',\n",
    "    1: 'spleen',\n",
    "    2: 'kidney',\n",
    "    3: 'bowel'\n",
    "}\n",
    "organ_index_dict = {v: k for k, v in organ_index_dict_inv.items()}\n",
    "\n",
    "# labels dict (for BE models)\n",
    "label_index_dict_inv = {\n",
    "    0: 'bowel',\n",
    "    1: 'extravasation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_series_from_dataset(dir_: str, max_slices: Optional[int]=None)-> np.ndarray:\n",
    "    \"\"\"seriesを読み込む.\"\"\"\n",
    "    path_list = os.listdir(dir_)\n",
    "    path_list = [[int(path.replace(\".npy\",\"\")), path] for path in path_list]\n",
    "    path_list.sort()\n",
    "    path_list = [path[1] for path in path_list]\n",
    "    if max_slices is not None:\n",
    "        step = (len(path_list) + max_slices - 1) // max_slices\n",
    "        path_list = path_list[::step]\n",
    "    arr = []\n",
    "    for path in path_list:\n",
    "        arr.append(np.load(os.path.join(dir_, path)))\n",
    "    return np.array(arr)\n",
    "\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\"画像の読み込み.\n",
    "    Args:\n",
    "        path (str): 画像のパス.\n",
    "    Returns:\n",
    "        numpy.ndarray: 画像.\n",
    "    Note:\n",
    "        現在読み込む画像の形式は.png, .npy, .npzのみ対応.\n",
    "        cv2.IMREAD_UNCHANGED: 16bit画像やアルファチャンネルを考慮した読み込み.\n",
    "    \"\"\"\n",
    "    if path.endswith(\".png\"):\n",
    "        image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    elif path.endswith(\".npy\"):\n",
    "        image = np.load(path)\n",
    "    elif path.endswith(\".npz\"):\n",
    "        image = np.load(path)[\"arr_0\"]\n",
    "    else:\n",
    "        raise Exception(f\"unexpected image format: {path}\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    \"\"\"推論パイプライン.\"\"\"\n",
    "    def __init__(self,CFG_INF: Any, CFG_LSK: Any, CFG_BE: Any):\n",
    "        self.CFG_INF = CFG_INF\n",
    "        self.CFG_LSK = CFG_LSK\n",
    "        self.CFG_BE = CFG_BE\n",
    "\n",
    "        self.lsk_models = cls_load_models(CFG_LSK)\n",
    "        self.be_models = cls_load_models(CFG_BE, framework=\"timm\")\n",
    "    \n",
    "    def __call__(self, pid: int) -> tuple:\n",
    "        \"\"\"inference process.\n",
    "        1. load images from dicom files.\n",
    "        2. create segmentation masks.\n",
    "        3. create liver, spleen, kidney volumes.\n",
    "        4. inference lsk models.\n",
    "        5. inference be models.\n",
    "        Args:\n",
    "            pid (int): patient id.\n",
    "        Return example:\n",
    "            dict: {\n",
    "            'pid': 0,\n",
    "            'bowel_healthy': 0.0,\n",
    "            'bowel_injury': 0.0,\n",
    "            'extravasation_healthy': 0.0,\n",
    "            'extravasation_injury': 0.0,\n",
    "            'kidney_healthy': 0.0,\n",
    "            'kidney_low': 0.0,\n",
    "            'kidney_high': 0.0,\n",
    "            'liver_healthy': 0.0,\n",
    "            'liver_low': 0.0,\n",
    "            'liver_high': 0.0,\n",
    "            'spleen_healthy': 0.0,\n",
    "            'spleen_low': 0.0,\n",
    "            'spleen_high': 0.0\n",
    "            }\n",
    "        Note:\n",
    "            - １症例に複数シリーズ存在する場合、各シリーズに対して推論を行い、全予測結果の最大値を採用する.\n",
    "            - 推論時間的に厳しければ、最初のシリーズのみを採用するなど検討.\n",
    "        \"\"\"\n",
    "        df_study = self.CFG_INF.df_series_meta[self.CFG_INF.df_series_meta['patient_id']==pid].reset_index(drop=True)\n",
    "        # df_study内のそれぞれのシリーズを取得して、画像枚数に対して降順にソート.\n",
    "        df_study = self.get_slices_and_sort(df_study)\n",
    "        preds = defaultdict(list)\n",
    "        for sid in df_study['series_id'].to_list()[:self.CFG_INF.max_series]:\n",
    "            data = self.load_data(pid, sid)\n",
    "            if data is None:\n",
    "                continue\n",
    "            lsk_preds = self.lsk_prediction(pid, sid)\n",
    "            be_preds = self.be_prediction(data)\n",
    "            for idx, organ in organ_index_dict_inv.items():\n",
    "                if idx == 3:\n",
    "                    continue\n",
    "                preds[organ].append(lsk_preds[idx])\n",
    "            for idx, label in label_index_dict_inv.items():\n",
    "                pred = np.array([be_preds[idx]])\n",
    "                preds[label].append(pred)\n",
    "\n",
    "        ret = {'patient_id': pid}\n",
    "        for k,v in preds.items():\n",
    "            v = np.array(v)\n",
    "            ret[k] = np.max(v, axis=0)\n",
    "        ret = self.convert_submission_format(ret)\n",
    "        return ret\n",
    "\n",
    "    def load_data(self, pid: int, sid: int)-> np.ndarray:\n",
    "        \"\"\"dicomから画像を読み込む.\n",
    "        Args:\n",
    "            pid (int): patient id.\n",
    "            sid (int): series id.\n",
    "        Returns:\n",
    "            np.ndarray: (Z, H, W) normalized CT series.\n",
    "        Note:\n",
    "            - preprocessは全モデル共通なので、ここで行う.\n",
    "            - H, Wはすべてself.CFG_INF.image_sizeにresizeされる.\n",
    "        \"\"\"\n",
    "        series_path = os.path.join(self.CFG_BE.image_dir, 'train_images', str(pid), str(sid))\n",
    "        # sample submissionでこういう例が存在する.\n",
    "        if not os.path.exists(series_path):  \n",
    "            return None\n",
    "        image_arr = load_series_from_dataset(series_path, self.CFG_INF.max_slices)\n",
    "        image_arr = apply_preprocess(image_arr, resize=self.CFG_INF.image_size)\n",
    "        # sample submission対応\n",
    "        if len(image_arr) < self.CFG_INF.min_slices:\n",
    "            image_arr = resize_1d(image_arr, self.CFG_INF.min_slices, axis=0)\n",
    "        return image_arr\n",
    "    \n",
    "    def get_slices_and_sort(self, df_study: pd.DataFrame)-> pd.DataFrame:\n",
    "        \"\"\"シリーズのスライス数を取得して、スライス数に対して降順にソートする.\n",
    "        Args:\n",
    "            df_study (pd.DataFrame): series meta dataframe.\n",
    "        Returns:\n",
    "            pd.DataFrame: sorted series meta dataframe.\n",
    "        \"\"\"\n",
    "        pid = df_study['patient_id'][0]\n",
    "        df_study['n_slices'] = 0\n",
    "        for i in range(len(df_study)):\n",
    "            sid = df_study['series_id'][i]\n",
    "            series_path = os.path.join(self.CFG_INF.image_dir, str(pid), str(sid))\n",
    "            if os.path.exists(series_path):\n",
    "                df_study['n_slices'][i] = len(os.listdir(series_path))\n",
    "        df_study = df_study.sort_values(by='n_slices', ascending=False)\n",
    "        return df_study\n",
    "    \n",
    "    def lsk_prediction(self, pid: int, sid: int)-> np.ndarray:\n",
    "        \"\"\"liver, spleen, kidneyの予測値を返す.\n",
    "        Args:\n",
    "            pid: patient id\n",
    "            sid: series id\n",
    "        Returns:\n",
    "            np.ndarray: (organs, grades).\n",
    "        \"\"\"\n",
    "        volumes = self.get_lsk_volumes(pid, sid) # (organs, z, h, w)\n",
    "        volumes = apply_preprocess(volumes)\n",
    "        lsk_iterator = self.pseudo_iterator(self.CFG_LSK, volumes)\n",
    "        pred = cls_inference(self.CFG_LSK, self.lsk_models, lsk_iterator)\n",
    "        return pred\n",
    "\n",
    "    def get_lsk_volumes(self, pid: int, sid: int)->Dict[str, np.ndarray]:\n",
    "        \"\"\"Segmentationからliver, spleen, kidneyのvolume dataを作成.\n",
    "        Args:\n",
    "            pid: patient id\n",
    "            sid: series id\n",
    "        Returns:\n",
    "            np.ndarray: (organs, z, h, w).\n",
    "        Note:\n",
    "            - organsはliver, spleen, kidneyの順番.\n",
    "            - この関数内でCFG.LSK.image_sizeのreshapeまで行う.\n",
    "            - 腎臓は左右を分離してからくっつけ直すという特殊な処理が必要.\n",
    "        \"\"\"\n",
    "        arr = []\n",
    "        for idx, organ in organ_index_dict_inv.items():\n",
    "            if idx == 3:\n",
    "                continue\n",
    "            path = os.path.join(self.CFG_LSK.image_dir, str(pid), str(sid), f\"{organ}.npy\")\n",
    "            if organ == \"kidney\":\n",
    "                # 解剖学的な左右を、画像上の左右に置き換えて読み込み\n",
    "                l, r = (\n",
    "                    path.replace(\"kidney.npy\", \"kidney_r.npy\"),\n",
    "                    path.replace(\"kidney.npy\", \"kidney_l.npy\"),\n",
    "                )\n",
    "                if os.path.exists(l):\n",
    "                    l = load_image(l)\n",
    "                else:\n",
    "                    l = np.zeros(self.CFG_LSK.image_size)\n",
    "                if os.path.exists(r):\n",
    "                    r = load_image(r)\n",
    "                else:\n",
    "                    r = np.zeros(self.CFG_LSK.image_size)\n",
    "                img_cropped = kidney_specific(self.CFG_LSK, l, r)\n",
    "            else:\n",
    "                organ_segment = load_image(path)\n",
    "                img_cropped = resize_3d(organ_segment, self.CFG_LSK.image_size)\n",
    "                \n",
    "            arr.append(img_cropped)\n",
    "        arr = np.stack(arr, axis=0)\n",
    "        return arr\n",
    "    \n",
    "    def be_prediction(self, data: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"bowel_injury及びextravasation_injuryの予測を行う.\n",
    "        Args:\n",
    "            data: (Z, H, W).\n",
    "        Returns:\n",
    "            np.ndarray: [bowel_injury_pred, extravasation_injury_pred].\n",
    "            example: [0.1, 0.9].\n",
    "        \"\"\"\n",
    "        be_iterator = self.pseudo_iterator(self.CFG_BE, data)\n",
    "        pred = cls_inference(self.CFG_BE, self.be_models, be_iterator)\n",
    "        pred = self.be_prediction_postprocess(pred)\n",
    "        return pred\n",
    "    \n",
    "    def be_prediction_postprocess(self, pred: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"スライスごとの予測をシリーズの予測に変換する.\n",
    "        Args:\n",
    "            pred: (len(data),['bowel_injury', 'extravasation_injury']).\n",
    "        Returns:\n",
    "            np.ndarray: ['bowel_injury', 'extravasation_injury'].\n",
    "        Note:\n",
    "            - 予測値の最大値から外れ値を考慮した2%percentileを採用する.\n",
    "        \"\"\"\n",
    "        bowel = pred[:, 0]\n",
    "        extravasation = pred[:, 1]\n",
    "        p = 90\n",
    "        bowel = np.percentile(bowel, p)\n",
    "        extravasation = np.percentile(extravasation, p)\n",
    "        return np.array([bowel, extravasation])\n",
    "\n",
    "    def pseudo_iterator(self, CFG: Any, images: np.ndarray)-> tuple:\n",
    "        \"\"\"evaluation iterator.\n",
    "        Args:\n",
    "            CFG: config.\n",
    "            images: (batch dim, H, W) or (batch dim, Z, H, W).\n",
    "        \"\"\"\n",
    "        batch = CFG.batch_size\n",
    "        for i in range(0, len(images), batch):\n",
    "            arr = images[i : i + batch]\n",
    "            arr = self.add_ch_dim(arr)\n",
    "            arr = torch.from_numpy(arr.astype(arr.dtype, copy=False))\n",
    "            yield arr\n",
    "\n",
    "    def add_ch_dim(self, images: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"1次元目にchannel dimを追加する.\"\"\"\n",
    "        return images[:, np.newaxis, ...]\n",
    "\n",
    "    def convert_submission_format(self, pred: dict)->dict:\n",
    "        \"\"\"提出形式に変換する.\"\"\"\n",
    "        converted = dict()\n",
    "        for idx, organ in organ_index_dict_inv.items():\n",
    "            if idx == 3:\n",
    "                continue\n",
    "            for idx, grade in enumerate(['healthy', 'low', 'high']):\n",
    "                converted[f'{organ}_{grade}'] = pred[organ][idx]\n",
    "        for idx, label in label_index_dict_inv.items():\n",
    "            converted[f'{label}_healthy'] = 1 - pred[label][0]\n",
    "            converted[f'{label}_injury'] = pred[label][0]\n",
    "\n",
    "        converted['patient_id'] = pred['patient_id']\n",
    "        return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solid_organ = load_df(CFG_LSK)\n",
    "# fold 0のpatient_idを取得\n",
    "pids = df_solid_organ[df_solid_organ[\"fold\"] == 0][\"patient_id\"].unique()\n",
    "df_all = pd.read_csv(os.path.join(CFG_INF.base_dir, 'train.csv'))\n",
    "train_pids = df_solid_organ[df_solid_organ[\"fold\"] != 0][\"patient_id\"].unique()\n",
    "valid_pids = df_solid_organ[df_solid_organ[\"fold\"] == 0][\"patient_id\"].unique()\n",
    "df_train = df_all[df_all[\"patient_id\"].isin(train_pids)].reset_index(drop=True)\n",
    "df_valid = df_all[df_all[\"patient_id\"].isin(valid_pids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance = Inference(CFG_INF, CFG_LSK, CFG_BE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline(exp016)   \n",
    "cv: 0.678  \n",
    "cv (weighted mean): 0.602  \n",
    "### 2シリーズあった場合、maxではなくmeanにする  \n",
    "cv: 0.6770   \n",
    "cv (wm): 0.6002  \n",
    "\n",
    "### bowel, extraivasationを95%タイルに\n",
    "cv: 0.7058\n",
    "cv (wm): 0.5905\n",
    "\n",
    "### bowel, extraivasationを90%タイルに  \n",
    "cv: 0.7391\n",
    "cv (wm): 0.5979\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "results = []\n",
    "for pid in tqdm(df_valid['patient_id'].to_list()[:3]):\n",
    "    result = inference_instance(pid)\n",
    "    results.append(result)\n",
    "    ct += 1\n",
    "    if ct == 10**8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# resultsを保存\n",
    "dir_ = os.path.join(CFG_INF.model_save_dir, CFG_INF.exp_name)\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "path = os.path.join(dir_, \"results.pkl\")\n",
    "# with open(path, 'wb') as f:\n",
    "#    pickle.dump(results, f)\n",
    "\n",
    "# resultsを読み込み\n",
    "with open(path, 'rb') as f:\n",
    "   results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(results)\n",
    "order = CFG_INF.df.columns.tolist()\n",
    "if \"any_injury\" in order:\n",
    "    order.remove(\"any_injury\")\n",
    "submission = submission[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.1947\n",
      "extravasation: 0.8315\n",
      "kidney: 0.5702\n",
      "liver: 0.6323\n",
      "spleen: 0.7709\n",
      "any_injury: 1.2529\n",
      "mean: 0.7088\n",
      "Training score without scaling: 0.7088\n"
     ]
    }
   ],
   "source": [
    "# add weight\n",
    "solution_train = create_training_solution(df_valid)\n",
    "\n",
    "no_scale_score = score(solution_train.copy(),submission.copy(),'patient_id')\n",
    "print(f'Training score without scaling: {no_scale_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.2901\n",
      "extravasation: 0.6081\n",
      "kidney: 0.5731\n",
      "liver: 0.6353\n",
      "spleen: 0.6997\n",
      "any_injury: 0.8062\n",
      "mean: 0.6021\n",
      "Training score with weight scaling: 0.6021\n"
     ]
    }
   ],
   "source": [
    "# Group by different sample weights\n",
    "scale_by_2 = ['bowel_injury','kidney_low','liver_low','spleen_low']\n",
    "scale_by_4 = ['kidney_high','liver_high','spleen_high']\n",
    "scale_by_6 = ['extravasation_injury']\n",
    "\n",
    "# Scale factors based on described metric \n",
    "sf_2 = 2\n",
    "sf_4 = 4\n",
    "sf_6 = 6\n",
    "\n",
    "# Reset the prediction\n",
    "y_pred = submission.copy()\n",
    "\n",
    "# Scale each target \n",
    "y_pred[scale_by_2] *=sf_2\n",
    "y_pred[scale_by_4] *=sf_4\n",
    "y_pred[scale_by_6] *=sf_6\n",
    "\n",
    "weight_scale_score = score(solution_train.copy(),y_pred.copy(),'patient_id')\n",
    "print(f'Training score with weight scaling: {weight_scale_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_hack(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"bowel, liver, spleen, kidney.いずれかが高い予測値を持つとき、extravasationも合併している確率が高い.\"\"\"\n",
    "    cols = [\"extravasation_injury\", \"bowel_injury\", \"liver_low\", \"liver_high\", \"spleen_low\", \"spleen_high\", \"kidney_low\", \"kidney_high\"]\n",
    "    df[\"extravasation_injury\"] = df[cols].max(axis=1)\n",
    "    df[\"extravasation_healthy\"] = 1 - df[\"extravasation_injury\"]\n",
    "    return df\n",
    "submission = metric_hack(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.1947\n",
      "extravasation: 0.6501\n",
      "kidney: 0.5702\n",
      "liver: 0.6323\n",
      "spleen: 0.7709\n",
      "any_injury: 1.2529\n",
      "mean: 0.6785\n",
      "Training score without scaling: 0.6785\n"
     ]
    }
   ],
   "source": [
    "# add weight\n",
    "solution_train = create_training_solution(df_valid)\n",
    "\n",
    "no_scale_score = score(solution_train.copy(),submission.copy(),'patient_id')\n",
    "print(f'Training score without scaling: {no_scale_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.2901\n",
      "extravasation: 0.7066\n",
      "kidney: 0.5731\n",
      "liver: 0.6353\n",
      "spleen: 0.6997\n",
      "any_injury: 0.7079\n",
      "mean: 0.6021\n",
      "Training score with weight scaling: 0.6021\n"
     ]
    }
   ],
   "source": [
    "# Group by different sample weights\n",
    "scale_by_2 = ['bowel_injury','kidney_low','liver_low','spleen_low']\n",
    "scale_by_4 = ['kidney_high','liver_high','spleen_high']\n",
    "scale_by_6 = ['extravasation_injury']\n",
    "\n",
    "# Scale factors based on described metric \n",
    "sf_2 = 2\n",
    "sf_4 = 4\n",
    "sf_6 = 6\n",
    "\n",
    "# Reset the prediction\n",
    "y_pred = submission.copy()\n",
    "\n",
    "# Scale each target \n",
    "y_pred[scale_by_2] *=sf_2\n",
    "y_pred[scale_by_4] *=sf_4\n",
    "y_pred[scale_by_6] *=sf_6\n",
    "\n",
    "weight_scale_score = score(solution_train.copy(),y_pred.copy(),'patient_id')\n",
    "print(f'Training score with weight scaling: {weight_scale_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.1319\n",
      "extravasation: 0.6081\n",
      "kidney: 0.5731\n",
      "liver: 0.6687\n",
      "spleen: 0.6997\n",
      "any_injury: 0.8614\n",
      "mean: 0.5905\n",
      "Training score with weight scaling: 0.5905\n"
     ]
    }
   ],
   "source": [
    "# Reset the prediction\n",
    "y_pred = submission.copy()\n",
    "\n",
    "y_pred['bowel_injury'] *= 0.3\n",
    "y_pred['kidney_low'] *= 2\n",
    "y_pred['liver_low'] *= 2\n",
    "y_pred['spleen_low'] *= 2\n",
    "y_pred['kidney_high'] *= 4\n",
    "y_pred['liver_high'] *= 4\n",
    "y_pred['spleen_high'] *= 4\n",
    "y_pred['extravasation_injury'] *= 6\n",
    "\n",
    "weight_scale_score = score(solution_train.copy(),y_pred.copy(),'patient_id')\n",
    "print(f'Training score with weight scaling: {weight_scale_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
