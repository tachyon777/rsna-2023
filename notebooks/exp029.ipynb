{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp029\n",
    "[Notion](https://www.notion.so/exp029-4f4f45b75231433ea84a337562c3851b?pvs=4)  \n",
    "実際の推論パイプラインを再確認し、cv-lbの乖離の原因がバグではないことを確認する.  \n",
    "Copy from: exp012.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Any, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# リポジトリtopに移動\n",
    "while os.path.basename(os.getcwd()) != 'rsna-2023':\n",
    "    os.chdir('../')\n",
    "    if os.getcwd() == '/':\n",
    "        raise Exception('Could not find project root directory.')\n",
    "    \n",
    "from src.data_io import load_dicom_series\n",
    "from src.segmentation.model import load_models as seg_load_models\n",
    "from src.segmentation.trainer import inference as seg_inference\n",
    "from src.classification.dataset import load_df\n",
    "from src.image_processing import apply_preprocess, crop_organ, kidney_split, resize_volume, apply_postprocess, kidney_specific, resize_3d, resize_1d\n",
    "from src.classification.model import load_models as cls_load_models\n",
    "from src.classification.trainer import inference as cls_inference\n",
    "from src.metrics import score, create_training_solution, normalize_probabilities_to_one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG_INF:\n",
    "    exp_name = 'exp_029'\n",
    "    # evaluation時：'train', submission時：'test'\n",
    "    phase = 'train'\n",
    "    base_dir = 'data/rsna-2023-abdominal-trauma-detection'\n",
    "    image_dir = f'data/rsna-2023-abdominal-trauma-detection/{phase}_images'\n",
    "    # dataframeはこのconfigにもたせ、phaseで対応できるようにする.\n",
    "    if phase == 'train':\n",
    "        df = pd.read_csv(os.path.join(base_dir, 'train.csv'))\n",
    "    elif phase == 'test':\n",
    "        df = pd.read_csv(os.path.join(base_dir, 'sample_submission.csv'))\n",
    "    df_series_meta = pd.read_csv(os.path.join(base_dir, f'{phase}_series_meta.csv'))\n",
    "    image_size = (512, 512)\n",
    "    # sample submissionで極端にスライス数が少ない場合があるため対応.\n",
    "    min_slices = 10\n",
    "    # 推論時間制限のため\n",
    "    max_slices = 500\n",
    "    max_series = 2\n",
    "    model_save_dir = \"outputs\"\n",
    "    seg_model_mode = 'final'\n",
    "    lsk_model_mode = 'final'\n",
    "    be_model_mode = 'final'\n",
    "\n",
    "class CFG_SEG:\n",
    "    exp_name = 'exp_004'\n",
    "    # model config\n",
    "    backbone = 'efficientnet-b3'\n",
    "    n_ch = 1\n",
    "    expand_ch_dim = False\n",
    "    n_class = 4 # 学習時は腎臓の左右を区別しないので、5->4\n",
    "    # hyper params\n",
    "    init_lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    image_size = (512, 512)\n",
    "    batch_size = 32\n",
    "    amp = True\n",
    "    n_epoch = 20\n",
    "    iteration_per_epoch = 200\n",
    "    pretrain = True\n",
    "    freeze_epochs = 1\n",
    "    noaug_epochs = 1\n",
    "    # fold config\n",
    "    n_fold = 6\n",
    "    include_evaluation = False\n",
    "    train_folds = 1\n",
    "    # path\n",
    "    image_dir = \"data/dataset001/train_images\"\n",
    "    mask_dir = \"data/dataset001/segmentations\"\n",
    "    model_save_dir = \"outputs\"\n",
    "    # other config\n",
    "    seed = 42\n",
    "    num_workers = 0\n",
    "    num_gpus = 2\n",
    "    progress_bar = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CFG_LSK:\n",
    "    exp_name = 'exp_024'\n",
    "    # model config\n",
    "    # timm backbone\n",
    "    backbone = 'efficientnet-b4'\n",
    "    n_ch = 1\n",
    "    expand_ch_dim = True\n",
    "    # n_class: healthy, low, high\n",
    "    n_class = 3\n",
    "    # hyper params\n",
    "    init_lr = 1e-4\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    image_size = (128, 128, 160)\n",
    "    batch_size = 64\n",
    "    amp = True\n",
    "    eps = 1e-6\n",
    "    n_epoch = 20\n",
    "    pretrain = True\n",
    "    freeze_epochs = 1\n",
    "    noaug_epochs = 1\n",
    "    # fold config\n",
    "    n_fold = 6\n",
    "    include_evaluation = False\n",
    "    train_folds = 1\n",
    "    # path\n",
    "    image_dir = \"data/dataset002\"\n",
    "    model_save_dir = \"outputs\"\n",
    "    # other config\n",
    "    seed = 42\n",
    "    num_workers = 4\n",
    "    num_gpus = 2\n",
    "    progress_bar = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    class_weight=torch.tensor([1.0, 2.0, 4.0]).to(device)\n",
    "\n",
    "class CFG_BE:\n",
    "    exp_name = 'exp_026'\n",
    "    # model config\n",
    "    # timm backbone\n",
    "    backbone = 'efficientnet_b4'\n",
    "    # n_ch: z軸方向のスライス数\n",
    "    n_ch = 5\n",
    "    expand_ch_dim = False\n",
    "    # n_class: bowel_injury, extravasation\n",
    "    # sample weighted: 4class\n",
    "    n_class = 4\n",
    "    label_smoothing = None #Optional(float)\n",
    "    # hyper params\n",
    "    init_lr = 5e-5\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    image_size = (512, 512)\n",
    "    batch_size = 64\n",
    "    amp = True\n",
    "    eps = 1e-7\n",
    "    n_epoch = 18\n",
    "    iteration_per_epoch = 100\n",
    "    pretrain = True\n",
    "    freeze_epochs = 1\n",
    "    noaug_epochs = 2\n",
    "    # fold config\n",
    "    n_fold = 6\n",
    "    include_evaluation = False\n",
    "    train_folds = 1\n",
    "    # path\n",
    "    image_dir = \"data/dataset001\"\n",
    "    model_save_dir = \"outputs\"\n",
    "    # other config\n",
    "    seed = 42\n",
    "    num_workers = 4\n",
    "    num_gpus = 2\n",
    "    progress_bar = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # b_healty, b_injury, e_healty, e_injury\n",
    "    class_weight=torch.tensor([1.0, 2.0, 1.0, 6.0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organs dict (for SEG and LSK models)\n",
    "organ_index_dict_inv = {\n",
    "    0: 'liver',\n",
    "    1: 'spleen',\n",
    "    2: 'kidney',\n",
    "    3: 'bowel'\n",
    "}\n",
    "organ_index_dict = {v: k for k, v in organ_index_dict_inv.items()}\n",
    "\n",
    "# labels dict (for BE models)\n",
    "label_index_dict_inv = {\n",
    "    0: 'bowel',\n",
    "    1: 'extravasation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    \"\"\"推論パイプライン.\"\"\"\n",
    "    def __init__(self,CFG_INF: Any, CFG_SEG: Any, CFG_LSK: Any, CFG_BE: Any):\n",
    "        self.CFG_INF = CFG_INF\n",
    "        self.CFG_SEG = CFG_SEG\n",
    "        self.CFG_LSK = CFG_LSK\n",
    "        self.CFG_BE = CFG_BE\n",
    "\n",
    "        self.seg_models = seg_load_models(CFG_SEG, mode=self.CFG_INF.seg_model_mode)\n",
    "        self.lsk_models = cls_load_models(CFG_LSK, mode=self.CFG_INF.lsk_model_mode)\n",
    "        self.be_models = cls_load_models(CFG_BE, mode=self.CFG_INF.be_model_mode, framework=\"timm\")\n",
    "    \n",
    "    def __call__(self, pid: int) -> tuple:\n",
    "        \"\"\"inference process.\n",
    "        1. load images from dicom files.\n",
    "        2. create segmentation masks.\n",
    "        3. create liver, spleen, kidney volumes.\n",
    "        4. inference lsk models.\n",
    "        5. inference be models.\n",
    "        Args:\n",
    "            pid (int): patient id.\n",
    "        Return example:\n",
    "            dict: {\n",
    "            'pid': 0,\n",
    "            'bowel_healthy': 0.0,\n",
    "            'bowel_injury': 0.0,\n",
    "            'extravasation_healthy': 0.0,\n",
    "            'extravasation_injury': 0.0,\n",
    "            'kidney_healthy': 0.0,\n",
    "            'kidney_low': 0.0,\n",
    "            'kidney_high': 0.0,\n",
    "            'liver_healthy': 0.0,\n",
    "            'liver_low': 0.0,\n",
    "            'liver_high': 0.0,\n",
    "            'spleen_healthy': 0.0,\n",
    "            'spleen_low': 0.0,\n",
    "            'spleen_high': 0.0\n",
    "            }\n",
    "        Note:\n",
    "            - １症例に複数シリーズ存在する場合、各シリーズに対して推論を行い、全予測結果の最大値を採用する.\n",
    "            - 推論時間的に厳しければ、最初のシリーズのみを採用するなど検討.\n",
    "        \"\"\"\n",
    "        df_study = self.CFG_INF.df_series_meta[self.CFG_INF.df_series_meta['patient_id']==pid].reset_index(drop=True)\n",
    "        # df_study内のそれぞれのシリーズを取得して、画像枚数に対して降順にソート.\n",
    "        df_study = self.get_slices_and_sort(df_study)\n",
    "        preds = defaultdict(list)\n",
    "        for sid in df_study['series_id'].to_list()[:self.CFG_INF.max_series]:\n",
    "            data = self.load_data(pid, sid)\n",
    "            if data is None:\n",
    "                continue\n",
    "            lsk_preds = self.lsk_prediction(data)\n",
    "            be_preds = self.be_prediction(data)\n",
    "            \n",
    "            for idx, organ in organ_index_dict_inv.items():\n",
    "                if idx == 3:\n",
    "                    continue\n",
    "                preds[organ].append(lsk_preds[idx])\n",
    "            for idx, label in label_index_dict_inv.items():\n",
    "                pred = np.array([be_preds[idx]])\n",
    "                preds[label].append(pred)\n",
    "\n",
    "        ret = {'patient_id': pid}\n",
    "        for k,v in preds.items():\n",
    "            v = np.array(v)\n",
    "            ret[k] = np.max(v, axis=0)\n",
    "        ret = self.convert_submission_format(ret)\n",
    "        return ret\n",
    "\n",
    "    def load_data(self, pid: int, sid: int)-> np.ndarray:\n",
    "        \"\"\"dicomから画像を読み込む.\n",
    "        Args:\n",
    "            pid (int): patient id.\n",
    "            sid (int): series id.\n",
    "        Returns:\n",
    "            np.ndarray: (Z, H, W) normalized CT series.\n",
    "        Note:\n",
    "            - preprocessは全モデル共通なので、ここで行う.\n",
    "            - H, Wはすべてself.CFG_INF.image_sizeにresizeされる.\n",
    "        \"\"\"\n",
    "        series_path = os.path.join(self.CFG_INF.image_dir, str(pid), str(sid))\n",
    "        # sample submissionでこういう例が存在する.\n",
    "        if not os.path.exists(series_path): \n",
    "            return None\n",
    "        image_arr, path_list, meta_list = load_dicom_series(series_path, self.CFG_INF.max_slices)\n",
    "        image_arr = apply_preprocess(image_arr, resize=self.CFG_INF.image_size)\n",
    "        # sample submission対応\n",
    "        if len(image_arr) < self.CFG_INF.min_slices:\n",
    "            image_arr = resize_1d(image_arr, self.CFG_INF.min_slices, axis=0)\n",
    "        return image_arr\n",
    "    \n",
    "    def get_slices_and_sort(self, df_study: pd.DataFrame)-> pd.DataFrame:\n",
    "        \"\"\"シリーズのスライス数を取得して、スライス数に対して降順にソートする.\n",
    "        Args:\n",
    "            df_study (pd.DataFrame): series meta dataframe.\n",
    "        Returns:\n",
    "            pd.DataFrame: sorted series meta dataframe.\n",
    "        \"\"\"\n",
    "        pid = df_study['patient_id'][0]\n",
    "        df_study['n_slices'] = 0\n",
    "        for i in range(len(df_study)):\n",
    "            sid = df_study['series_id'][i]\n",
    "            series_path = os.path.join(self.CFG_INF.image_dir, str(pid), str(sid))\n",
    "            if os.path.exists(series_path):\n",
    "                df_study['n_slices'][i] = len(os.listdir(series_path))\n",
    "        df_study = df_study.sort_values(by='n_slices', ascending=False)\n",
    "        return df_study\n",
    "    \n",
    "    def lsk_prediction(self, data: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"liver, spleen, kidneyの予測値を返す.\n",
    "        Args:\n",
    "            data: (Z, H, W).\n",
    "        Returns:\n",
    "            np.ndarray: (organs, grades).\n",
    "        \"\"\"\n",
    "        volumes = self.get_lsk_volumes(data) # (organs, z, h, w)\n",
    "        lsk_iterator = self.pseudo_iterator(self.CFG_LSK, volumes)\n",
    "        pred = cls_inference(self.CFG_LSK, self.lsk_models, lsk_iterator)\n",
    "        return pred\n",
    "\n",
    "    def get_lsk_volumes(self, data: np.ndarray)->Dict[str, np.ndarray]:\n",
    "        \"\"\"Segmentationからliver, spleen, kidneyのvolume dataを作成.\n",
    "        Args:\n",
    "            data: (Z, H, W).\n",
    "        Returns:\n",
    "            np.ndarray: (organs, z, h, w).\n",
    "        Note:\n",
    "            - organsはliver, spleen, kidneyの順番.\n",
    "            - この関数内でCFG.LSK.image_sizeのreshapeまで行う.\n",
    "            - 腎臓は左右を分離してからくっつけ直すという特殊な処理が必要.\n",
    "        \"\"\"\n",
    "        masks = self.get_segmentation(data)\n",
    "        masks = apply_postprocess(self.CFG_SEG, masks)\n",
    "        arr = []\n",
    "        for idx, organ in organ_index_dict_inv.items():\n",
    "            if idx == 3:\n",
    "                continue\n",
    "            organ_segment = masks[..., idx]\n",
    "            if organ_segment.sum() == 0:\n",
    "                arr.append(np.zeros(self.CFG_LSK.image_size))\n",
    "                continue\n",
    "            img_cropped, mask_cropped = crop_organ(data, organ_segment)\n",
    "            if organ == \"kidney\":\n",
    "                kidney_r, kidney_l = kidney_split(img_cropped, mask_cropped)\n",
    "                img_cropped = kidney_specific(self.CFG_LSK, kidney_r, kidney_l)\n",
    "            else:\n",
    "                img_cropped = resize_3d(img_cropped, self.CFG_LSK.image_size)\n",
    "            arr.append(img_cropped)\n",
    "        arr = np.stack(arr, axis=0)\n",
    "        return arr\n",
    "    \n",
    "    def get_segmentation(self, data: np.ndarray)->np.ndarray:\n",
    "        \"\"\"Segmentation modelを使って、各臓器のマスクを作成.\n",
    "        Args:\n",
    "            data: (Z, H, W).\n",
    "        Returns:\n",
    "            mask: (z, h, w, ch) binarized.\"\"\"\n",
    "        seg_iterator = self.pseudo_iterator(self.CFG_SEG, data)\n",
    "        pred = seg_inference(self.CFG_SEG, self.seg_models, seg_iterator)\n",
    "        pred = (pred > 0.5).astype(np.uint8)\n",
    "        return pred\n",
    "    \n",
    "    def be_prediction(self, data: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"bowel_injury及びextravasation_injuryの予測を行う.\n",
    "        Args:\n",
    "            data: (Z, H, W).\n",
    "        Returns:\n",
    "            np.ndarray: [bowel_injury_pred, extravasation_injury_pred].\n",
    "            example: [0.1, 0.9].\n",
    "        \"\"\"\n",
    "        be_iterator = self.pseudo_iterator(self.CFG_BE, data)\n",
    "        pred = cls_inference(self.CFG_BE, self.be_models, be_iterator)\n",
    "        pred = self.be_prediction_postprocess(pred)\n",
    "        return pred\n",
    "    \n",
    "    def be_prediction_postprocess(self, pred: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"スライスごとの予測をシリーズの予測に変換する.\n",
    "        Args:\n",
    "            pred: (len(data),['bowel_injury', 'extravasation_injury']).\n",
    "        Returns:\n",
    "            np.ndarray: ['bowel_injury', 'extravasation_injury'].\n",
    "        Note:\n",
    "            - 予測値の最大値から外れ値を考慮した2%percentileを採用する.\n",
    "        \"\"\"\n",
    "        if self.CFG_BE.n_class == 1:\n",
    "            bowel = np.zeros_like(pred[:, 0])\n",
    "            extravasation = pred[:, 0]\n",
    "        elif self.CFG_BE.n_class == 2:\n",
    "            bowel = pred[:, 0]\n",
    "            extravasation = pred[:, 1]\n",
    "        elif self.CFG_BE.n_class == 4:\n",
    "            bowel = normalize_probabilities_to_one(pred[:, :2])\n",
    "            extravasation = normalize_probabilities_to_one(pred[:, 2:])\n",
    "            bowel = bowel[:, 1]\n",
    "            extravasation = extravasation[:, 1]\n",
    "        p = 98\n",
    "        bowel = np.percentile(bowel, p)\n",
    "        extravasation = np.percentile(extravasation, p)\n",
    "        return np.array([bowel, extravasation])\n",
    "\n",
    "    def pseudo_iterator(self, CFG: Any, images: np.ndarray)-> tuple:\n",
    "        \"\"\"evaluation iterator.\n",
    "        Args:\n",
    "            CFG: config.\n",
    "            images: (batch dim, H, W) or (batch dim, Z, H, W).\n",
    "        \"\"\"\n",
    "        batch = CFG.batch_size\n",
    "        length = len(images)\n",
    "        arr = []\n",
    "        if not CFG.expand_ch_dim:\n",
    "            images = self.add_dummy_array(CFG, images)\n",
    "        for i in range(length):\n",
    "            if CFG.expand_ch_dim:\n",
    "                img = images[i]\n",
    "                img = img[np.newaxis, ...]\n",
    "            else:\n",
    "                img = images[i:i+CFG.n_ch]\n",
    "            arr.append(img)\n",
    "            if i != 0 and (i%batch==0 or i == length-1):\n",
    "                arr = np.stack(arr, axis=0)\n",
    "                arr = torch.from_numpy(arr.astype(arr.dtype, copy=False))\n",
    "                yield arr\n",
    "                arr = []\n",
    "    \n",
    "    def add_dummy_array(self, CFG: Any, images: np.ndarray)-> np.ndarray:\n",
    "        \"\"\"chが複数ある場合に、事前に0配列を追加しておく.\"\"\"\n",
    "        add_ch = CFG.n_ch//2\n",
    "        arr = []\n",
    "        img = np.zeros_like(images[0])\n",
    "        for i in range(add_ch):\n",
    "            arr.append(img)\n",
    "        arr.extend(images)\n",
    "        for i in range(add_ch):\n",
    "            arr.append(img)\n",
    "        arr = np.stack(arr, axis=0)\n",
    "        return arr\n",
    "\n",
    "    def convert_submission_format(self, pred: dict)->dict:\n",
    "        \"\"\"提出形式に変換する.\"\"\"\n",
    "        converted = dict()\n",
    "        for idx, organ in organ_index_dict_inv.items():\n",
    "            if idx == 3:\n",
    "                continue\n",
    "            for idx, grade in enumerate(['healthy', 'low', 'high']):\n",
    "                converted[f'{organ}_{grade}'] = pred[organ][idx]\n",
    "        for idx, label in label_index_dict_inv.items():\n",
    "            converted[f'{label}_healthy'] = 1 - pred[label][0]\n",
    "            converted[f'{label}_injury'] = pred[label][0]\n",
    "\n",
    "        converted['patient_id'] = pred['patient_id']\n",
    "        return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solid_organ = load_df(CFG_LSK)\n",
    "# fold 0のpatient_idを取得\n",
    "pids = df_solid_organ[df_solid_organ[\"fold\"] == 0][\"patient_id\"].unique()\n",
    "df_all = pd.read_csv(os.path.join(CFG_INF.base_dir, 'train.csv'))\n",
    "train_pids = df_solid_organ[df_solid_organ[\"fold\"] != 0][\"patient_id\"].unique()\n",
    "valid_pids = df_solid_organ[df_solid_organ[\"fold\"] == 0][\"patient_id\"].unique()\n",
    "df_train = df_all[df_all[\"patient_id\"].isin(train_pids)].reset_index(drop=True)\n",
    "df_valid = df_all[df_all[\"patient_id\"].isin(valid_pids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance = Inference(CFG_INF, CFG_SEG, CFG_LSK, CFG_BE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [3:20:38<00:00, 22.93s/it]  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for pid in tqdm(df_valid['patient_id'].to_list()):\n",
    "    result = inference_instance(pid)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# resultsを保存\n",
    "dir_ = os.path.join(CFG_INF.model_save_dir, CFG_INF.exp_name)\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "path = os.path.join(dir_, \"results.pkl\")\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# resultsを読み込み\n",
    "# with open(path, 'rb') as f:\n",
    "#    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(results)\n",
    "order = CFG_INF.df.columns.tolist()\n",
    "if \"any_injury\" in order:\n",
    "    order.remove(\"any_injury\")\n",
    "submission = submission[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007</td>\n",
       "      <td>0.816518</td>\n",
       "      <td>0.183482</td>\n",
       "      <td>0.947678</td>\n",
       "      <td>0.052322</td>\n",
       "      <td>0.951286</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>0.905826</td>\n",
       "      <td>0.060036</td>\n",
       "      <td>0.033985</td>\n",
       "      <td>0.941805</td>\n",
       "      <td>0.036837</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10205</td>\n",
       "      <td>0.779929</td>\n",
       "      <td>0.220071</td>\n",
       "      <td>0.946795</td>\n",
       "      <td>0.053205</td>\n",
       "      <td>0.872198</td>\n",
       "      <td>0.078430</td>\n",
       "      <td>0.053869</td>\n",
       "      <td>0.919182</td>\n",
       "      <td>0.054013</td>\n",
       "      <td>0.024664</td>\n",
       "      <td>0.928935</td>\n",
       "      <td>0.046155</td>\n",
       "      <td>0.023084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10275</td>\n",
       "      <td>0.956565</td>\n",
       "      <td>0.043435</td>\n",
       "      <td>0.965257</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>0.953950</td>\n",
       "      <td>0.034398</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.944327</td>\n",
       "      <td>0.085028</td>\n",
       "      <td>0.038267</td>\n",
       "      <td>0.970852</td>\n",
       "      <td>0.041635</td>\n",
       "      <td>0.021986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10430</td>\n",
       "      <td>0.956799</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.844909</td>\n",
       "      <td>0.155091</td>\n",
       "      <td>0.864793</td>\n",
       "      <td>0.135347</td>\n",
       "      <td>0.130747</td>\n",
       "      <td>0.858291</td>\n",
       "      <td>0.108673</td>\n",
       "      <td>0.066383</td>\n",
       "      <td>0.894300</td>\n",
       "      <td>0.077345</td>\n",
       "      <td>0.039788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10494</td>\n",
       "      <td>0.816477</td>\n",
       "      <td>0.183523</td>\n",
       "      <td>0.916104</td>\n",
       "      <td>0.083896</td>\n",
       "      <td>0.927674</td>\n",
       "      <td>0.054439</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.942114</td>\n",
       "      <td>0.037052</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.957192</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.008788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n",
       "0       10007       0.816518      0.183482               0.947678   \n",
       "1       10205       0.779929      0.220071               0.946795   \n",
       "2       10275       0.956565      0.043435               0.965257   \n",
       "3       10430       0.956799      0.043201               0.844909   \n",
       "4       10494       0.816477      0.183523               0.916104   \n",
       "\n",
       "   extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n",
       "0              0.052322        0.951286    0.029996     0.012964   \n",
       "1              0.053205        0.872198    0.078430     0.053869   \n",
       "2              0.034743        0.953950    0.034398     0.011485   \n",
       "3              0.155091        0.864793    0.135347     0.130747   \n",
       "4              0.083896        0.927674    0.054439     0.019471   \n",
       "\n",
       "   liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n",
       "0       0.905826   0.060036    0.033985        0.941805    0.036837   \n",
       "1       0.919182   0.054013    0.024664        0.928935    0.046155   \n",
       "2       0.944327   0.085028    0.038267        0.970852    0.041635   \n",
       "3       0.858291   0.108673    0.066383        0.894300    0.077345   \n",
       "4       0.942114   0.037052    0.006877        0.957192    0.028234   \n",
       "\n",
       "   spleen_high  \n",
       "0     0.016900  \n",
       "1     0.023084  \n",
       "2     0.021986  \n",
       "3     0.039788  \n",
       "4     0.008788  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.1935\n",
      "extravasation: 0.8347\n",
      "kidney: 0.5443\n",
      "liver: 0.6181\n",
      "spleen: 0.6675\n",
      "any_injury: 1.1834\n",
      "mean: 0.6736\n",
      "Training score without scaling: 0.6736\n"
     ]
    }
   ],
   "source": [
    "# add weight\n",
    "solution_train = create_training_solution(df_valid)\n",
    "\n",
    "no_scale_score = score(solution_train.copy(),submission.copy(),'patient_id')\n",
    "print(f'Training score without scaling: {no_scale_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowel: 0.1478\n",
      "extravasation: 0.5995\n",
      "kidney: 0.5112\n",
      "liver: 0.6342\n",
      "spleen: 0.6176\n",
      "any_injury: 0.7761\n",
      "mean: 0.5477\n",
      "Training score with weight scaling: 0.5477\n"
     ]
    }
   ],
   "source": [
    "# Group by different sample weights\n",
    "scale_by_1 = ['bowel_injury']\n",
    "scale_by_2 = ['kidney_low','liver_low','spleen_low']\n",
    "scale_by_4 = ['kidney_high','liver_high','spleen_high']\n",
    "scale_by_6 = ['extravasation_injury']\n",
    "\n",
    "# Scale factors based on described metric \n",
    "sf_1 = 0.3\n",
    "sf_2 = 2\n",
    "sf_4 = 4\n",
    "sf_6 = 6\n",
    "\n",
    "# Reset the prediction\n",
    "y_pred = submission.copy()\n",
    "\n",
    "# Scale each target \n",
    "y_pred[scale_by_1] *=sf_1\n",
    "y_pred[scale_by_2] *=sf_2\n",
    "y_pred[scale_by_4] *=sf_4\n",
    "y_pred[scale_by_6] *=sf_6\n",
    "\n",
    "weight_scale_score = score(solution_train.copy(),y_pred.copy(),'patient_id')\n",
    "print(f'Training score with weight scaling: {weight_scale_score:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
